{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d0536ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import _init_paths\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from model.faster_rcnn.vgg16 import vgg16\n",
    "from model.utils.config import cfg, cfg_from_list\n",
    "#from datasets.factory import get_imdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bda5200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17952\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64c7b1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': array([[712, 143, 810, 307]], dtype=uint16),\n",
       " 'gt_classes': array([2], dtype=int32),\n",
       " 'gt_ishard': array([0], dtype=int32),\n",
       " 'gt_overlaps': <1x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 1 stored elements in Compressed Sparse Row format>,\n",
       " 'flipped': False,\n",
       " 'seg_areas': array([16335.], dtype=float32),\n",
       " 'img_id': 0,\n",
       " 'image': '/home/superorange5/Research/FedWCD/data/KITTI/JPEGImages/000000.png',\n",
       " 'width': 1224,\n",
       " 'height': 370,\n",
       " 'max_classes': array([2]),\n",
       " 'max_overlaps': array([1.], dtype=float32)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for datainfo in data:\n",
    "    wh_pair =[(1242,375),(1224,370),(1238,374),(1241,376)]\n",
    "\n",
    "    width = datainfo['width']\n",
    "    height = datainfo['height']\n",
    "    temp = (width,height)\n",
    "    if (temp not in wh_pair):\n",
    "        print(datainfo['image'])\n",
    "        print (width)\n",
    "        print (height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125cd274",
   "metadata": {},
   "source": [
    "# WK value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c377819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "testimg_pickle_path = \"data/pickle/BDD_val_1000.pkl\"\n",
    "with open(testimg_pickle_path, 'rb') as handle:\n",
    "    test_images = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd73454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_classes =  ('__background__', 'car', 'person','rider', 'train', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "178c23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(imdb_classes,model_path, lr):\n",
    "    \n",
    "    fasterRCNN = vgg16(imdb_classes, pretrained=False, class_agnostic=False)\n",
    "    fasterRCNN.create_architecture()\n",
    "    fasterRCNN.cuda()\n",
    "    \n",
    "    \n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    \n",
    "    fasterRCNN.load_state_dict(checkpoint['model'])\n",
    "    \n",
    "    params = []\n",
    "    DOUBLE_BIAS = True\n",
    "    WEIGHT_DECAY = 0.0005\n",
    "    BIAS_DECAY = False\n",
    "\n",
    "    for key, value in dict(fasterRCNN.named_parameters()).items():\n",
    "        if value.requires_grad:\n",
    "            if 'bias' in key:\n",
    "                params += [{'params':[value],'lr':lr*(DOUBLE_BIAS + 1), \\\n",
    "                    'weight_decay': BIAS_DECAY and WEIGHT_DECAY or 0}]\n",
    "            else:\n",
    "                params += [{'params':[value],'lr':lr, 'weight_decay': WEIGHT_DECAY}]\n",
    "    \n",
    "    optimizer = torch.optim.SGD(params, momentum=0.9) \n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return fasterRCNN,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21e70780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parties=1\n",
    "model_list=[None] * parties\n",
    "load_name = \"models/vgg16/multi_ck/multi_ck_FedAvg/faster_rcnn_multi_ck_AVG_10.pth\"\n",
    "for i in range(parties):\n",
    "    model_list[i], optimizer =load_model(imdb_classes, load_name,lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df58a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9eb69e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frcnn_helper import *\n",
    "\n",
    "def getWeight(test_images,model_list, batch_size, k):\n",
    "    \n",
    "    wk_list = []\n",
    "    for fasterRCNN in model_list:\n",
    "#         if args.mGPUs:\n",
    "#             fasterRCNN = fasterRCNN.module\n",
    "        X = get_features(fasterRCNN, test_images, batch_size)/255.0\n",
    "        wk_value = within_cluster_dispersion(X, n_cluster=k)\n",
    "        wk_list.append(wk_value)\n",
    "        print(wk_value)\n",
    "    \n",
    "    return wk_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bff51a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03316984123371074\n"
     ]
    }
   ],
   "source": [
    "batch_size=16\n",
    "k=5\n",
    "wk_list_curr = getWeight(test_images,model_list, batch_size,k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get within class dispersion        \n",
    "\n",
    "\n",
    "if i==1:\n",
    "    wk_diff = wk_list_curr\n",
    "else:\n",
    "    wk_diff=[]\n",
    "    for list1_c, list2_p in zip(wk_list_prev, wk_list_curr ):        \n",
    "        wk_diff.append(list1_c-list2_p)\n",
    "\n",
    "print('diff={}'.format(wk_diff))\n",
    "\n",
    "wk_ratio = softmax(wk_diff).tolist()    \n",
    "print('wk_ratio={}'.format(wk_ratio))\n",
    "\n",
    "#wk_ratio = [x / sum(wk_diff) for x in wk_diff]\n",
    "#keep wk to previous\n",
    "wk_list_prev = wk_list_curr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945f5d59",
   "metadata": {},
   "source": [
    "# BDD converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39693c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_path = '/work/superorange5/bdd100k/labels/bdd100k_labels_images_train.json'\n",
    "with open(json_path) as f:\n",
    "    j = f.read()\n",
    "    data = json.loads(j)\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "#     print(datum['name'])\n",
    "    \n",
    "# for datum in tqdm(data):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8002bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'car', 'attributes': {'occluded': False, 'truncated': True, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 732.648216, 'y1': 273.850561, 'x2': 1277.473049, 'y2': 615.293917}, 'id': 324596}\n",
      "{'category': 'car', 'attributes': {'occluded': False, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 545.59664, 'y1': 356.984595, 'x2': 679.204909, 'y2': 466.840282}, 'id': 324597}\n",
      "{'category': 'car', 'attributes': {'occluded': True, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 498.804206, 'y1': 382.020696, 'x2': 552.247514, 'y2': 421.113478}, 'id': 324598}\n",
      "{'category': 'car', 'attributes': {'occluded': True, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 485.443379, 'y1': 384.49491, 'x2': 521.072251, 'y2': 409.732027}, 'id': 324599}\n",
      "{'category': 'car', 'attributes': {'occluded': True, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 482.474305, 'y1': 378.556762, 'x2': 515.134105, 'y2': 402.309344}, 'id': 324600}\n",
      "{'category': 'car', 'attributes': {'occluded': True, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 434.969144, 'y1': 381.525836, 'x2': 479.505233, 'y2': 417.154708}, 'id': 324601}\n",
      "{'category': 'car', 'attributes': {'occluded': True, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 423.092854, 'y1': 378.556762, 'x2': 442.391826, 'y2': 403.793881}, 'id': 324602}\n",
      "{'category': 'car', 'attributes': {'occluded': True, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 405.278418, 'y1': 377.072227, 'x2': 427.546463, 'y2': 408.24749}, 'id': 324603}\n",
      "{'category': 'car', 'attributes': {'occluded': False, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 322.144382, 'y1': 342.927892, 'x2': 412.701099, 'y2': 429.030999}, 'id': 324604}\n",
      "{'category': 'car', 'attributes': {'occluded': False, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 175.175287, 'y1': 387.463982, 'x2': 225.649522, 'y2': 417.15471}, 'id': 324605}\n",
      "{'category': 'car', 'attributes': {'occluded': True, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 277.608293, 'y1': 372.618618, 'x2': 325.113456, 'y2': 399.340272}, 'id': 324606}\n",
      "{'category': 'car', 'attributes': {'occluded': False, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 258.309322, 'y1': 384.49491, 'x2': 305.814484, 'y2': 414.185636}, 'id': 324607}\n",
      "{'category': 'car', 'attributes': {'occluded': True, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 201.89694, 'y1': 372.618618, 'x2': 255.340248, 'y2': 403.793881}, 'id': 324608}\n"
     ]
    }
   ],
   "source": [
    "for datum in data:\n",
    "    if datum['name'] == '1df7314f-da89fc35.jpg':\n",
    "        for bb in datum['labels']:\n",
    "            if bb['category'] == 'car':\n",
    "                print(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a213d2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5382c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"val.txt\", \"w\")\n",
    "\n",
    "for datum in data:\n",
    "    if(datum['attributes']['timeofday'] =='daytime'):\n",
    "        fp.write(datum['name'].replace('.jpg','')+\"\\n\" )\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "446b14d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'attributes', 'timestamp', 'labels'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9004e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for datum in data:\n",
    "   \n",
    "    # bounding box\n",
    "    for label in datum['labels']:\n",
    "        classes = label['category']\n",
    "        if classes =='motor':\n",
    "            print(\"motorcycle\")\n",
    "        elif classes =='bike':\n",
    "            print(\"bicycle\")\n",
    "        elif classes =='traffic light':\n",
    "            print(\"skip\")\n",
    "        elif classes =='traffic sign':\n",
    "            print(\"skip\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930e448e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000f77c-6257be58.jpg'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['name']   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d920e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daytime'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['attributes']['timeofday']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd0a3d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'traffic light'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['labels'][0]['category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e1e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26c75299",
   "metadata": {},
   "source": [
    "## read XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a34ec179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79744\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "# assign directory\n",
    "#directory = 'data/KITTI/Annotations/'\n",
    "directory = 'data/BDD100K/Annotations/'\n",
    "count=0\n",
    "    # check exist car or not\n",
    "for filename in os.listdir(directory):\n",
    "    #print(filename)\n",
    "    \n",
    "    tree = ET.parse(os.path.join(directory,filename))\n",
    "    root = tree.getroot()\n",
    "\n",
    "    objs = tree.findall('object')\n",
    "\n",
    "\n",
    "            # Load object bounding boxes into a data frame.\n",
    "    num_objs =0\n",
    "    for ix, obj in enumerate(objs):\n",
    "        bbox = obj.find('bndbox')\n",
    "\n",
    "#         x1 = max(float(bbox.find('xmin').text), 0)\n",
    "#         y1 = max(float(bbox.find('ymin').text), 0)\n",
    "#         x2 = max(float(bbox.find('xmax').text), 0)\n",
    "#         y2 = max(float(bbox.find('ymax').text), 0)\n",
    "\n",
    "\n",
    "        class_name = obj.find('name').text.lower().strip()\n",
    "        if class_name=='train':            \n",
    "            num_objs+=1\n",
    "        #print(class_name)\n",
    "    #print(num_objs)\n",
    "    if (num_objs)==0:\n",
    "        #print(filename)\n",
    "        count+=1\n",
    "print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c497b7",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5b241c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5a8b758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/vgg16/multi_ck/faster_rcnn_1_20_17897.pth\"\n",
    "model  = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "08df7b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vgg16(\n",
       "  (RCNN_rpn): _RPN(\n",
       "    (RPN_Conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (RPN_cls_score): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (RPN_bbox_pred): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (RPN_proposal): _ProposalLayer()\n",
       "    (RPN_anchor_target): _AnchorTargetLayer()\n",
       "  )\n",
       "  (RCNN_proposal_target): _ProposalTargetLayer()\n",
       "  (RCNN_roi_pool): ROIPool(output_size=(7, 7), spatial_scale=0.0625)\n",
       "  (RCNN_roi_align): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0)\n",
       "  (RCNN_base): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "  )\n",
       "  (RCNN_top): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "  )\n",
       "  (RCNN_cls_score): Linear(in_features=4096, out_features=9, bias=True)\n",
       "  (RCNN_bbox_pred): Linear(in_features=4096, out_features=36, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.asarray(['__background__', 'car', 'person', 'bus', 'bicycle', 'motorcycle','rider', 'train', 'truck'])\n",
    "set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '20'] \n",
    "\n",
    "cfg_from_list(set_cfgs)\n",
    "fasterRCNN = vgg16(classes, pretrained=False, class_agnostic=False)\n",
    "fasterRCNN.create_architecture()\n",
    "fasterRCNN.eval()\n",
    "fasterRCNN.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4197efad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasterRCNN.load_state_dict(model['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e897a",
   "metadata": {},
   "source": [
    "## check xml class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0e824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/BDD100K/Annotations/002d290d-89f4e5c0.xml\n",
      "data/BDD100K/Annotations/02f95a54-6494531c.xml\n",
      "data/BDD100K/Annotations/0406b065-a759da9c.xml\n",
      "data/BDD100K/Annotations/04488248-af1d57c1.xml\n",
      "data/BDD100K/Annotations/06cdf0b1-6fec3924.xml\n",
      "data/BDD100K/Annotations/0710389f-038050ae.xml\n",
      "data/BDD100K/Annotations/0710389f-038050ae.xml\n",
      "data/BDD100K/Annotations/0710389f-038050ae.xml\n",
      "data/BDD100K/Annotations/0710389f-038050ae.xml\n",
      "data/BDD100K/Annotations/0c1d07e3-8cb61d13.xml\n",
      "data/BDD100K/Annotations/0c1d07e3-8cb61d13.xml\n",
      "data/BDD100K/Annotations/0ed0d326-5ec5573f.xml\n",
      "data/BDD100K/Annotations/1125356f-edb7f7c8.xml\n",
      "data/BDD100K/Annotations/13250071-2043b0ff.xml\n",
      "data/BDD100K/Annotations/148b9b8a-7d0771aa.xml\n",
      "data/BDD100K/Annotations/152b261b-776bfbb0.xml\n",
      "data/BDD100K/Annotations/1be335cc-3074a07c.xml\n",
      "data/BDD100K/Annotations/1c8b6894-dca9308f.xml\n",
      "data/BDD100K/Annotations/1df7314f-da89fc35.xml\n",
      "data/BDD100K/Annotations/1df7314f-da89fc35.xml\n",
      "data/BDD100K/Annotations/2124b66a-da9e92f8.xml\n",
      "data/BDD100K/Annotations/221fa2a1-4b37bd43.xml\n",
      "data/BDD100K/Annotations/2532100b-e3a8d68c.xml\n",
      "data/BDD100K/Annotations/2532100b-e3a8d68c.xml\n",
      "data/BDD100K/Annotations/2541a390-31023d52.xml\n",
      "data/BDD100K/Annotations/2541a390-31023d52.xml\n",
      "data/BDD100K/Annotations/2541a390-31023d52.xml\n",
      "data/BDD100K/Annotations/2541a390-31023d52.xml\n",
      "data/BDD100K/Annotations/2541a390-31023d52.xml\n",
      "data/BDD100K/Annotations/2541a390-31023d52.xml\n",
      "data/BDD100K/Annotations/2541a390-31023d52.xml\n",
      "data/BDD100K/Annotations/2541a390-31023d52.xml\n",
      "data/BDD100K/Annotations/2541a390-31023d52.xml\n",
      "data/BDD100K/Annotations/2541a390-31023d52.xml\n",
      "data/BDD100K/Annotations/2541a390-31023d52.xml\n",
      "data/BDD100K/Annotations/27642d00-53875a7e.xml\n",
      "data/BDD100K/Annotations/27642d00-53875a7e.xml\n",
      "data/BDD100K/Annotations/2d765c3b-78834627.xml\n",
      "data/BDD100K/Annotations/2d765c3b-78834627.xml\n",
      "data/BDD100K/Annotations/2f5b65f6-a449be4a.xml\n",
      "data/BDD100K/Annotations/2f5b65f6-a449be4a.xml\n",
      "data/BDD100K/Annotations/302e431c-227244ee.xml\n",
      "data/BDD100K/Annotations/320608e7-6da10836.xml\n",
      "data/BDD100K/Annotations/320608e7-6da10836.xml\n",
      "data/BDD100K/Annotations/32cf0c44-8785d8e6.xml\n",
      "data/BDD100K/Annotations/32e89c7b-b2e4fae2.xml\n",
      "data/BDD100K/Annotations/3a2c9062-0cb2922f.xml\n",
      "data/BDD100K/Annotations/3a2c9062-0cb2922f.xml\n",
      "data/BDD100K/Annotations/3a2c9062-0cb2922f.xml\n",
      "data/BDD100K/Annotations/3a2c9062-0cb2922f.xml\n",
      "data/BDD100K/Annotations/3a6ad793-4c5ecfce.xml\n",
      "data/BDD100K/Annotations/3a6ae2aa-397c6af6.xml\n",
      "data/BDD100K/Annotations/3e7ed6ad-87ee042e.xml\n",
      "data/BDD100K/Annotations/3ebef51a-f2c85c45.xml\n",
      "data/BDD100K/Annotations/410fcb09-c5226e17.xml\n",
      "data/BDD100K/Annotations/418c8b37-b221c63c.xml\n",
      "data/BDD100K/Annotations/45fd93e3-3daa78f1.xml\n",
      "data/BDD100K/Annotations/46f76df9-be3b359e.xml\n",
      "data/BDD100K/Annotations/47fa6a56-50c5f13f.xml\n",
      "data/BDD100K/Annotations/48fc204f-fc202fba.xml\n",
      "data/BDD100K/Annotations/5187c895-bac630fb.xml\n",
      "data/BDD100K/Annotations/53279429-2c27c968.xml\n",
      "data/BDD100K/Annotations/5d4f6bb7-576403f6.xml\n",
      "data/BDD100K/Annotations/5d4f6bb7-576403f6.xml\n",
      "data/BDD100K/Annotations/5e2a5777-504e9835.xml\n",
      "data/BDD100K/Annotations/62d39b9d-a3515182.xml\n",
      "data/BDD100K/Annotations/652f7310-5dbedb62.xml\n",
      "data/BDD100K/Annotations/6788ef76-73309c5a.xml\n",
      "data/BDD100K/Annotations/68e7781a-cffc2268.xml\n",
      "data/BDD100K/Annotations/6b08751f-a29dcf6a.xml\n",
      "data/BDD100K/Annotations/6cc6d77b-6f1824a8.xml\n",
      "data/BDD100K/Annotations/6ebc6d1d-32aadf6f.xml\n",
      "data/BDD100K/Annotations/6fe61342-71024279.xml\n",
      "data/BDD100K/Annotations/71c52c0f-774abd3c.xml\n",
      "data/BDD100K/Annotations/72fdde58-64cbde72.xml\n",
      "data/BDD100K/Annotations/7f041bf7-7c01eeaf.xml\n",
      "data/BDD100K/Annotations/81398165-a23210fd.xml\n",
      "data/BDD100K/Annotations/845169ca-87f8f1e1.xml\n",
      "data/BDD100K/Annotations/87e78191-8ec98982.xml\n",
      "data/BDD100K/Annotations/8888d767-49dbc4e5.xml\n",
      "data/BDD100K/Annotations/8a761798-2250e90e.xml\n",
      "data/BDD100K/Annotations/8ab15e87-0bed87cd.xml\n",
      "data/BDD100K/Annotations/8d1398c8-bd5dd2f1.xml\n",
      "data/BDD100K/Annotations/8dab29d7-184dd4bf.xml\n",
      "data/BDD100K/Annotations/8dacce2d-5524b1be.xml\n",
      "data/BDD100K/Annotations/8edff929-24b4cefe.xml\n",
      "data/BDD100K/Annotations/91627650-595afffe.xml\n",
      "data/BDD100K/Annotations/95ef64ef-f33f5167.xml\n",
      "data/BDD100K/Annotations/99c2a804-aee080ad.xml\n",
      "data/BDD100K/Annotations/9c1ea83b-8bf937bc.xml\n",
      "data/BDD100K/Annotations/9e10ef26-3ba7ffe7.xml\n",
      "data/BDD100K/Annotations/a4f2b992-29aa8efd.xml\n",
      "data/BDD100K/Annotations/a6a59997-61c9ff61.xml\n",
      "data/BDD100K/Annotations/aa60b8fc-a9bfa4c6.xml\n",
      "data/BDD100K/Annotations/afc3456c-b6b9a65d.xml\n",
      "data/BDD100K/Annotations/b054fc2f-dcbb8dd4.xml\n",
      "data/BDD100K/Annotations/b089a76c-8a86bddf.xml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_annotation(x):\n",
    "    filename = os.path.join('data/BDD100K','Annotations',x +'.xml')\n",
    "    #print(filename)\n",
    "    tree = ET.parse(filename)\n",
    "    objs = tree.findall('object')\n",
    "\n",
    "    for ix, obj in enumerate(objs):\n",
    "        bbox = obj.find('bndbox')\n",
    "        cls=obj.find('name').text.lower().strip()\n",
    "        if(cls=='train'):\n",
    "            print(filename)\n",
    "        if('motorbike' == cls):\n",
    "            continue\n",
    "        #cls = _class_to_ind[obj.find('name').text.lower().strip()]\n",
    "    return filename\n",
    "\n",
    "#image_set_file = os.path.join('data/SIM10K/VOC2012', 'ImageSets', 'Main',  'trainval10k.txt')\n",
    "image_set_file = os.path.join('data/BDD100K', 'ImageSets', 'Main',  'train.txt')\n",
    "classes = ('__background__', 'car', 'person', 'bus', 'bicycle', 'motorcycle','rider', 'train', 'truck')\n",
    "num_classes = len(classes)\n",
    "_class_to_ind = dict(zip(classes, range(num_classes)))\n",
    "with open(image_set_file) as f:\n",
    "    for x in f.readlines():\n",
    "        x = x.rstrip(\"\\n\")\n",
    "        read_annotation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72226b04",
   "metadata": {},
   "source": [
    "## voc_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "568d4f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_cache_path = 'data/cache'\n",
    "pkl_file = os.path.join(data_cache_path, 'kitti_train' + '_gt_roidb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9ea957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ada3050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11968"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9548bfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': array([[829, 161, 888, 286],\n",
       "        [758, 178, 795, 271]], dtype=uint16),\n",
       " 'gt_classes': array([2, 2], dtype=int32),\n",
       " 'gt_ishard': array([0, 0], dtype=int32),\n",
       " 'gt_overlaps': <2x6 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 2 stored elements in Compressed Sparse Row format>,\n",
       " 'flipped': False,\n",
       " 'seg_areas': array([7560., 3572.], dtype=float32),\n",
       " 'img_id': 3180,\n",
       " 'image': '/home/superorange5/Research/FedWCD/data/KITTI/JPEGImages/003955.png',\n",
       " 'width': 1242,\n",
       " 'height': 375,\n",
       " 'max_classes': array([2, 2]),\n",
       " 'max_overlaps': array([1., 1.], dtype=float32)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee17f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "devkit_path = '/work/superorange5/bdd100k'\n",
    "cachedir = os.path.join(devkit_path, 'annotations_cache')\n",
    "image_set='val'\n",
    "imagesetfile = os.path.join(devkit_path,  'ImageSets',\n",
    "            'Main',   image_set + '.txt')\n",
    "cachefile = os.path.join(cachedir, '%s_annots.pkl' % imagesetfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e289422",
   "metadata": {},
   "outputs": [],
   "source": [
    "annopath = os.path.join(devkit_path, 'Annotations', '{:s}.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251210ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(imagesetfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "imagenames = [x.strip() for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609b540c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/superorange5/bdd100k/ImageSets/Main/val.txt_annots.pkl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "858ebae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imagenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d61edecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.datasets.voc_eval as voc_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579bb0fb",
   "metadata": {},
   "source": [
    "## generate gt pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00cb4e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cached annotations to /work/superorange5/bdd100k/ImageSets/Main/val.txt_annots.pkl\n"
     ]
    }
   ],
   "source": [
    "recs = {}\n",
    "#for i in range(1000):\n",
    "#    imagename = imagenames[i]\n",
    "for i, imagename in enumerate(imagenames):\n",
    "    recs[imagename] = voc_eval.parse_rec(annopath.format(imagename))\n",
    "    if i % 100 == 0:\n",
    "        print('Reading annotation for {:d}/{:d}'.format(\n",
    "          i + 1, len(imagenames)))\n",
    "print('Saving cached annotations to {:s}'.format(cachefile))\n",
    "with open(cachefile, 'wb') as f:\n",
    "        pickle.dump(recs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9895ae9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'traffic light', 'truncated': 0, 'bbox': [449, 123, 471, 146]},\n",
       " {'name': 'traffic sign', 'truncated': 0, 'bbox': [638, 206, 653, 231]},\n",
       " {'name': 'traffic sign', 'truncated': 0, 'bbox': [199, 179, 231, 212]},\n",
       " {'name': 'car', 'truncated': 0, 'bbox': [439, 222, 698, 430]},\n",
       " {'name': 'car', 'truncated': 1, 'bbox': [1136, 200, 1278, 333]},\n",
       " {'name': 'motor', 'truncated': 1, 'bbox': [1204, 232, 1277, 330]}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs['b1ca8418-84a133a0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375dbfb",
   "metadata": {},
   "source": [
    "## extract gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bdfa8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classname = 'car' \n",
    "class_recs = {}\n",
    "npos = 0\n",
    "ovthresh=0.5\n",
    "for imagename in imagenames:\n",
    "    R = [obj for obj in recs[imagename] if obj['name'] == classname]\n",
    "    bbox = np.array([x['bbox'] for x in R])\n",
    "    #difficult = np.array([x['difficult'] for x in R]).astype(np.bool)\n",
    "    det = [False] * len(R)\n",
    "    npos = npos +  len(R) #sum(~difficult)\n",
    "    class_recs[imagename] = {'bbox': bbox,\n",
    "     #                        'difficult': difficult,\n",
    "                             'det': det}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e7371",
   "metadata": {},
   "source": [
    "## read dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e0367c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detfile = os.path.join(devkit_path,'results/Main/comp4_det_val_'+classname+'.txt')\n",
    "with open(detfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "splitlines = [x.strip().split(' ') for x in lines]\n",
    "image_ids = [x[0] for x in splitlines]\n",
    "confidence = np.array([float(x[1]) for x in splitlines])\n",
    "BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n",
    "\n",
    "nd = len(image_ids)\n",
    "tp = np.zeros(nd)\n",
    "fp = np.zeros(nd)\n",
    "\n",
    "if BB.shape[0] > 0:\n",
    "    # sort by confidence\n",
    "    sorted_ind = np.argsort(-confidence)\n",
    "    sorted_scores = np.sort(-confidence)\n",
    "    BB = BB[sorted_ind, :]\n",
    "    image_ids = [image_ids[x] for x in sorted_ind]\n",
    "\n",
    "    # go down dets and mark TPs and FPs\n",
    "    for d in range(nd):\n",
    "        R = class_recs[image_ids[d]]\n",
    "        bb = BB[d, :].astype(float)\n",
    "        ovmax = -np.inf\n",
    "        BBGT = R['bbox'].astype(float)\n",
    "\n",
    "        if BBGT.size > 0:\n",
    "        # compute overlaps\n",
    "        # intersection\n",
    "            ixmin = np.maximum(BBGT[:, 0], bb[0])\n",
    "            iymin = np.maximum(BBGT[:, 1], bb[1])\n",
    "            ixmax = np.minimum(BBGT[:, 2], bb[2])\n",
    "            iymax = np.minimum(BBGT[:, 3], bb[3])\n",
    "            iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
    "            ih = np.maximum(iymax - iymin + 1., 0.)\n",
    "            inters = iw * ih\n",
    "\n",
    "            # union\n",
    "            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n",
    "                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n",
    "                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n",
    "\n",
    "            overlaps = inters / uni\n",
    "            ovmax = np.max(overlaps)\n",
    "            jmax = np.argmax(overlaps)\n",
    "\n",
    "        if ovmax > ovthresh:\n",
    "        #if not R['difficult'][jmax]:\n",
    "            if not R['det'][jmax]:\n",
    "                tp[d] = 1.\n",
    "                R['det'][jmax] = 1\n",
    "            else:\n",
    "                fp[d] = 1.\n",
    "        else:\n",
    "            fp[d] = 1.\n",
    "\n",
    "  # compute precision recall\n",
    "fp = np.cumsum(fp)\n",
    "tp = np.cumsum(tp)\n",
    "rec = tp / float(npos)\n",
    "  # avoid divide by zero in case the first detection matches a difficult\n",
    "  # ground truth\n",
    "prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "ap = voc_eval.voc_ap(rec, prec, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cccbfcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2416113877975409"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dee77289",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(cachefile):\n",
    "    # load annotations\n",
    "    recs = {}\n",
    "    for i, imagename in enumerate(imagenames):\n",
    "      recs[imagename] = parse_rec(annopath.format(imagename))\n",
    "      if i % 10 == 0:\n",
    "        print('Reading annotation for {:d}/{:d}'.format(\n",
    "          i + 1, len(imagenames)))\n",
    "    # save\n",
    "    print('Saving cached annotations to {:s}'.format(cachefile))\n",
    "    with open(cachefile, 'wb') as f:\n",
    "      pickle.dump(recs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "021ad5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': array([[1001,  282, 1041,  327],\n",
       "        [ 215,  172,  275,  230],\n",
       "        [ 797,  313,  830,  342],\n",
       "        [ 653,  303,  685,  316],\n",
       "        [ 707,  312,  716,  328],\n",
       "        [ 626,  296,  636,  317],\n",
       "        [ 317,  289,  329,  307],\n",
       "        [ 271,  288,  309,  302],\n",
       "        [ 221,  301,  232,  312],\n",
       "        [ 206,  338,  282,  388],\n",
       "        [  47,  344,  130,  401],\n",
       "        [ 247,  344,  347,  396],\n",
       "        [   0,  337,   52,  403],\n",
       "        [ 650,  354,  664,  382],\n",
       "        [ 649,  367,  664,  388],\n",
       "        [ 712,  337,  726,  353],\n",
       "        [ 684,  357,  721,  393],\n",
       "        [ 706,  364,  734,  391],\n",
       "        [ 727,  366,  761,  401],\n",
       "        [ 749,  362,  809,  409],\n",
       "        [ 787,  358,  906,  425],\n",
       "        [ 880,  376,  960,  452],\n",
       "        [ 936,  336, 1212,  484],\n",
       "        [1204,  416, 1279,  530],\n",
       "        [ 603,  341,  610,  353],\n",
       "        [ 591,  328,  598,  342],\n",
       "        [ 529,  332,  537,  342],\n",
       "        [ 552,  356,  565,  368],\n",
       "        [ 574,  352,  586,  369],\n",
       "        [ 581,  357,  598,  378],\n",
       "        [ 596,  352,  636,  386],\n",
       "        [ 518,  356,  535,  368],\n",
       "        [ 502,  357,  518,  371],\n",
       "        [ 453,  353,  479,  373]], dtype=uint16),\n",
       " 'gt_classes': array([9, 9, 9, 9, 8, 8, 8, 9, 9, 1, 1, 1, 1, 3, 7, 8, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 8, 8, 8, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 'gt_ishard': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 'gt_overlaps': <34x11 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 34 stored elements in Compressed Sparse Row format>,\n",
       " 'flipped': False,\n",
       " 'seg_areas': array([ 1886.,  3599.,  1020.,   462.,   170.,   242.,   247.,   585.,\n",
       "          144.,  3927.,  4872.,  5353.,  3551.,   435.,   352.,   255.,\n",
       "         1406.,   812.,  1260.,  2928.,  8160.,  6237., 41273.,  8740.,\n",
       "          104.,   120.,    99.,   182.,   234.,   396.,  1435.,   234.,\n",
       "          255.,   567.], dtype=float32),\n",
       " 'img_id': 0,\n",
       " 'image': '/home/superorange5/Research/FedWCD/data/BDD100K/JPEGImages/b1c66a42-6f7d68ca.jpg',\n",
       " 'width': 1280,\n",
       " 'height': 720,\n",
       " 'max_classes': array([9, 9, 9, 9, 8, 8, 8, 9, 9, 1, 1, 1, 1, 3, 7, 8, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 8, 8, 8, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'max_overlaps': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "  cachefile = os.path.join(cachedir, '%s_annots.pkl' % imagesetfile)\n",
    "  # read list of images\n",
    "  with open(imagesetfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "  imagenames = [x.strip() for x in lines]\n",
    "\n",
    "  if not os.path.isfile(cachefile):\n",
    "    # load annotations\n",
    "    recs = {}\n",
    "    for i, imagename in enumerate(imagenames):\n",
    "      recs[imagename] = parse_rec(annopath.format(imagename))\n",
    "      if i % 100 == 0:\n",
    "        print('Reading annotation for {:d}/{:d}'.format(\n",
    "          i + 1, len(imagenames)))\n",
    "    # save\n",
    "    print('Saving cached annotations to {:s}'.format(cachefile))\n",
    "    with open(cachefile, 'wb') as f:\n",
    "      pickle.dump(recs, f)\n",
    "  else:\n",
    "    # load\n",
    "    with open(cachefile, 'rb') as f:\n",
    "      try:\n",
    "        recs = pickle.load(f)\n",
    "      except:\n",
    "        recs = pickle.load(f, encoding='bytes')\n",
    "\n",
    "  # extract gt objects for this class\n",
    "  class_recs = {}\n",
    "  npos = 0\n",
    "  for imagename in imagenames:\n",
    "    R = [obj for obj in recs[imagename] if obj['name'] == classname]\n",
    "    bbox = np.array([x['bbox'] for x in R])\n",
    "    #difficult = np.array([x['difficult'] for x in R]).astype(np.bool)\n",
    "    det = [False] * len(R)\n",
    "    #npos = npos + sum(~difficult)\n",
    "    class_recs[imagename] = {'bbox': bbox,\n",
    "     #                        'difficult': difficult,\n",
    "                             'det': det}\n",
    "\n",
    "  # read dets\n",
    "  detfile = detpath.format(classname)\n",
    "  with open(detfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "  splitlines = [x.strip().split(' ') for x in lines]\n",
    "  image_ids = [x[0] for x in splitlines]\n",
    "  confidence = np.array([float(x[1]) for x in splitlines])\n",
    "  BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n",
    "\n",
    "  nd = len(image_ids)\n",
    "  tp = np.zeros(nd)\n",
    "  fp = np.zeros(nd)\n",
    "\n",
    "  if BB.shape[0] > 0:\n",
    "    # sort by confidence\n",
    "    sorted_ind = np.argsort(-confidence)\n",
    "    sorted_scores = np.sort(-confidence)\n",
    "    BB = BB[sorted_ind, :]\n",
    "    image_ids = [image_ids[x] for x in sorted_ind]\n",
    "\n",
    "    # go down dets and mark TPs and FPs\n",
    "    for d in range(nd):\n",
    "      R = class_recs[image_ids[d]]\n",
    "      bb = BB[d, :].astype(float)\n",
    "      ovmax = -np.inf\n",
    "      BBGT = R['bbox'].astype(float)\n",
    "\n",
    "      if BBGT.size > 0:\n",
    "        # compute overlaps\n",
    "        # intersection\n",
    "        ixmin = np.maximum(BBGT[:, 0], bb[0])\n",
    "        iymin = np.maximum(BBGT[:, 1], bb[1])\n",
    "        ixmax = np.minimum(BBGT[:, 2], bb[2])\n",
    "        iymax = np.minimum(BBGT[:, 3], bb[3])\n",
    "        iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
    "        ih = np.maximum(iymax - iymin + 1., 0.)\n",
    "        inters = iw * ih\n",
    "\n",
    "        # union\n",
    "        uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n",
    "               (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n",
    "               (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n",
    "\n",
    "        overlaps = inters / uni\n",
    "        ovmax = np.max(overlaps)\n",
    "        jmax = np.argmax(overlaps)\n",
    "\n",
    "      if ovmax > ovthresh:\n",
    "        #if not R['difficult'][jmax]:\n",
    "        if not R['det'][jmax]:\n",
    "          tp[d] = 1.\n",
    "          R['det'][jmax] = 1\n",
    "        else:\n",
    "          fp[d] = 1.\n",
    "      else:\n",
    "        fp[d] = 1.\n",
    "\n",
    "  # compute precision recall\n",
    "  fp = np.cumsum(fp)\n",
    "  tp = np.cumsum(tp)\n",
    "  rec = tp / float(npos)\n",
    "  # avoid divide by zero in case the first detection matches a difficult\n",
    "  # ground truth\n",
    "  prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "  ap = voc_ap(rec, prec, use_07_metric)\n",
    "\n",
    "  return rec, prec, ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3409553a",
   "metadata": {},
   "source": [
    "## bg_num_rois = 0 and fg_num_rois = 0, this should not happen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "76e148ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkl_filepath = 'data/cache/multi_ck_train_gt_roidb.pkl'\n",
    "with open(pkl_filepath, 'rb') as f:\n",
    "    multick_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2976686c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17898"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(multick_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "899f2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filepath = 'data/cache/kitti_train_gt_roidb.pkl'\n",
    "with open(pkl_filepath, 'rb') as f:\n",
    "    kiti_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cc082ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17952"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kiti_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "508ce363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': array([[414, 143, 512, 307]], dtype=uint16),\n",
       " 'gt_overlaps': <1x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 1 stored elements in Compressed Sparse Row format>,\n",
       " 'gt_classes': array([2], dtype=int32),\n",
       " 'flipped': True}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiti_data[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "57ae0b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': array([[414, 143, 512, 307]], dtype=uint16),\n",
       " 'gt_overlaps': <1x9 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 1 stored elements in Compressed Sparse Row format>,\n",
       " 'gt_classes': array([2], dtype=int32),\n",
       " 'flipped': True,\n",
       " 'img_id': 11914,\n",
       " 'image': '/home/superorange5/Research/FedWCD/data/KITTI/JPEGImages/000000.png',\n",
       " 'width': 1224,\n",
       " 'height': 370,\n",
       " 'max_classes': array([2]),\n",
       " 'max_overlaps': array([1.], dtype=float32)}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multick_data[11914]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151f23e",
   "metadata": {},
   "source": [
    "## roidb rank width error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc9dae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cache_path = 'data/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b3e3a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkl_filepath = 'data/cache/kitti_train_gt_roidb.pkl'\n",
    "with open(pkl_filepath, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c0b46f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitti_train\n"
     ]
    }
   ],
   "source": [
    "for s in 'kitti_train'.split('+'):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f4cd8",
   "metadata": {},
   "source": [
    "### rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d551abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = os.path.join(data_cache_path, 'kitti_train_small_gt_roidb.pkl')\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    roidb = pickle.load(f)\n",
    "\n",
    "#roidb = filter_roidb(roidb)\n",
    "train_size = len(roidb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b84d648e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f35ccf9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-8f753001ccaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroidb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11967\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "roidb[11967]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d05c4a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': array([[414, 143, 512, 307]], dtype=uint16),\n",
       " 'gt_overlaps': <1x3 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 1 stored elements in Compressed Sparse Row format>,\n",
       " 'gt_classes': array([2], dtype=int32),\n",
       " 'flipped': True}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roidb[11968]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "276903aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####0\n",
      "#####1\n",
      "#####2\n",
      "#####3\n",
      "#####4\n",
      "#####5\n",
      "#####6\n",
      "#####7\n",
      "#####8\n",
      "#####9\n",
      "#####10\n",
      "#####11\n",
      "#####12\n",
      "#####13\n",
      "#####14\n",
      "#####15\n",
      "#####16\n",
      "#####17\n",
      "#####18\n",
      "#####19\n",
      "#####20\n",
      "#####21\n",
      "#####22\n",
      "#####23\n",
      "#####24\n",
      "#####25\n",
      "#####26\n",
      "#####27\n",
      "#####28\n",
      "#####29\n",
      "#####30\n",
      "#####31\n",
      "#####32\n",
      "#####33\n",
      "#####34\n",
      "#####35\n",
      "#####36\n",
      "#####37\n",
      "#####38\n",
      "#####39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ratio_large = 2 # largest ratio to preserve.\n",
    "ratio_small = 0.5 # smallest ratio to preserve.    \n",
    "    \n",
    "ratio_list = []\n",
    " #   print(len(roidb))\n",
    "for i in range(len(roidb)):\n",
    "    print('#####'+str(i))\n",
    "      \n",
    "    width = roidb[i]['width']\n",
    "    height = roidb[i]['height']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f267bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_roidb(imdb_names, training=True):\n",
    "  \"\"\"\n",
    "  Combine multiple roidbs\n",
    "  \"\"\"\n",
    "\n",
    "  def get_training_roidb(imdb):\n",
    "    \"\"\"Returns a roidb (Region of Interest database) for use in training.\"\"\"\n",
    "    if cfg.TRAIN.USE_FLIPPED:\n",
    "      print('Appending horizontally-flipped training examples...')\n",
    "      imdb.append_flipped_images()\n",
    "      print('done')\n",
    "\n",
    "    print('Preparing training data...')\n",
    "\n",
    "    prepare_roidb(imdb)\n",
    "    #ratio_index = rank_roidb_ratio(imdb)\n",
    "    print('done')\n",
    "\n",
    "    return imdb.roidb\n",
    "  \n",
    "  def get_roidb(imdb_name):\n",
    "    imdb = get_imdb(imdb_name)\n",
    "    print('Loaded dataset `{:s}` for training'.format(imdb.name))\n",
    "    imdb.set_proposal_method(cfg.TRAIN.PROPOSAL_METHOD)\n",
    "    print('Set proposal method: {:s}'.format(cfg.TRAIN.PROPOSAL_METHOD))\n",
    "    roidb = get_training_roidb(imdb)\n",
    "    return roidb\n",
    "\n",
    "  roidbs = [get_roidb(s) for s in imdb_names.split('+')]\n",
    "  roidb = roidbs[0]\n",
    "  if len(roidbs) > 1:\n",
    "    for r in roidbs[1:]:\n",
    "      roidb.extend(r)\n",
    "    tmp = get_imdb(imdb_names.split('+')[1])\n",
    "    imdb = datasets.imdb.imdb(imdb_names, tmp.classes)\n",
    "  else:\n",
    "    imdb = get_imdb(imdb_names)\n",
    "\n",
    "  if training:\n",
    "    roidb = filter_roidb(roidb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a6ee3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(kiti_data)):\n",
    "  #    print('#####'+str(i))\n",
    "    width = multick_data[i]['width']\n",
    "    #print(width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729b847",
   "metadata": {},
   "source": [
    "## create a new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "25a74bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_multick = multick_data[0:100]+multick_data[2965:3065]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8dcd511c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_multick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "08b4c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file = 'data/cache/multi_ck_trainsmall'\n",
    "with open(cache_file, 'wb') as fid:\n",
    "    pickle.dump(new_multick, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_info in data:\n",
    "    bboxes = data_info['boxes']\n",
    "    if len(bboxes) <2:\n",
    "        print (data_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frcnn",
   "language": "python",
   "name": "frcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
